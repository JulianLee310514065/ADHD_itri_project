# Whisper åˆ†æ•£å¼èªéŸ³è½‰æ–‡å­—ç³»çµ±

ä¸€å€‹åŸºæ–¼ Flask + Docker çš„åˆ†æ•£å¼èªéŸ³è½‰æ–‡å­—ç³»çµ±ï¼Œåˆ©ç”¨å…§ç¶²æ¶æ§‹å°‡è¨ˆç®—ä»»å‹™åˆ†é…åˆ°é«˜æ€§èƒ½GPUæœå‹™å™¨ä¸Šè™•ç†ã€‚æ”¯æŒå¤šç¨®éŸ³é »/è¦–é »æ ¼å¼ï¼Œè‡ªå‹•è½‰æ›ç‚ºç¹é«”ä¸­æ–‡ã€‚

## ğŸ—ï¸ ç³»çµ±æ¶æ§‹

```
Internet â”€â”€â–º å‰ç«¯æœå‹™å™¨ (140.96.170.75:8496) â”€â”€â–º å¾Œç«¯æœå‹™å™¨ (192.168.50.120:8799)
             [æœ‰å°å¤–IPï¼Œç„¡GPU]                    [é«˜ç´šGPUï¼Œå…§ç¶²]
             Whisper_frontend                     whisper_backend
```

### æ¶æ§‹èªªæ˜

- **å‰ç«¯æœå‹™å™¨ (Frontend)**ï¼š
  - éƒ¨ç½²åœ¨æœ‰å°å¤–IPçš„æœå‹™å™¨ä¸Š (140.96.170.75:8496)
  - ä½œç‚ºä»£ç†æœå‹™å™¨ï¼Œæ¥æ”¶ç”¨æˆ¶è«‹æ±‚ä¸¦è½‰ç™¼åˆ°å¾Œç«¯
  - ä½¿ç”¨ Docker å®¹å™¨åŒ–éƒ¨ç½²ï¼Œæä¾› Web ç•Œé¢
  - ç„¡éœ€GPUï¼Œä¸»è¦è² è²¬è«‹æ±‚è™•ç†å’Œè½‰ç™¼

- **å¾Œç«¯æœå‹™å™¨ (Backend)**ï¼š
  - éƒ¨ç½²åœ¨å…§ç¶²é«˜æ€§èƒ½GPUæœå‹™å™¨ä¸Š (192.168.50.120:8799)
  - åŸ·è¡Œå¯¦éš›çš„èªéŸ³è½‰æ–‡å­—è™•ç† (ä½¿ç”¨ OpenAI Whisper large-v3 æ¨¡å‹)
  - ä½¿ç”¨ Waitress WSGI æœå‹™å™¨æä¾›ç©©å®šæœå‹™
  - è² è²¬é«˜è¨ˆç®—é‡çš„AIæ¨ç†å·¥ä½œï¼Œæ”¯æŒéšŠåˆ—è™•ç†

## ğŸ“ é …ç›®çµæ§‹

```
Whisper_fullend/
â”œâ”€â”€ Whisper_frontend/           # å‰ç«¯æœå‹™å™¨
â”‚   â”œâ”€â”€ Dockerfile             # Docker æ§‹å»ºæ–‡ä»¶
â”‚   â”œâ”€â”€ docker-compose.yml     # Docker Compose é…ç½®
â”‚   â”œâ”€â”€ requirements.txt       # Python ä¾è³´
â”‚   â”œâ”€â”€ whisper_trans_server.py # å‰ç«¯æœå‹™å™¨ä¸»ç¨‹åº
â”‚   â””â”€â”€ logs/                  # æ—¥èªŒç›®éŒ„
â””â”€â”€ whisper_backend/           # å¾Œç«¯æœå‹™å™¨
    â”œâ”€â”€ flask_server.py        # é–‹ç™¼ç”¨æœå‹™å™¨
    â”œâ”€â”€ waitress_server.py     # ç”Ÿç”¢ç”¨ Waitress æœå‹™å™¨
    â””â”€â”€ test.ipynb            # æ¸¬è©¦ç­†è¨˜æœ¬
```

## ğŸš€ å®‰è£éƒ¨ç½²

### å¾Œç«¯æœå‹™å™¨å®‰è£ (GPUæœå‹™å™¨)

1. **ç’°å¢ƒè¦æ±‚**ï¼š
   ```bash
   # Python 3.8+
   # CUDA æ”¯æŒçš„ GPU (å»ºè­° 8GB+ VRAM)
   # FFmpeg (ç”¨æ–¼è¦–é »éŸ³é »è™•ç†)
   ```

2. **å®‰è£ä¾è³´**ï¼š
   ```bash
   cd whisper_backend
   
   # å®‰è£ PyTorch (æ ¹æ“šä½ çš„ CUDA ç‰ˆæœ¬)
   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
   
   # å®‰è£å…¶ä»–ä¾è³´
   pip install openai-whisper flask waitress opencc-python-reimplemented
   
   # å®‰è£ FFmpeg (Ubuntu/Debian)
   sudo apt update
   sudo apt install ffmpeg
   ```

3. **å•Ÿå‹•å¾Œç«¯æœå‹™**ï¼š
   ```bash
   python waitress_server.py
   ```
   
   æœå‹™å°‡åœ¨ `http://192.168.50.120:8799` å•Ÿå‹•

### å‰ç«¯æœå‹™å™¨å®‰è£ (å°å¤–IPæœå‹™å™¨)

1. **Docker éƒ¨ç½² (æ¨è–¦)**ï¼š
   ```bash
   cd Whisper_frontend
   
   # ä½¿ç”¨ Docker Compose å•Ÿå‹•
   docker-compose up -d
   ```

2. **æ‰‹å‹•éƒ¨ç½²**ï¼š
   ```bash
   cd Whisper_frontend
   pip install -r requirements.txt
   python whisper_trans_server.py
   ```

## ğŸŒ ç¶²çµ¡é…ç½®

### å…§ç¶²è¨­å®š

ç¢ºä¿å‰ç«¯æœå‹™å™¨å¯ä»¥è¨ªå•å¾Œç«¯æœå‹™å™¨ï¼š

```bash
# åœ¨å‰ç«¯æœå‹™å™¨æ¸¬è©¦é€£æ¥
curl http://192.168.50.120:8799/health

# ä¿®æ”¹å¾Œç«¯URLï¼ˆå¦‚æœIPä¸åŒï¼‰
# ç·¨è¼¯ docker-compose.yml ä¸­çš„ BACKEND_URL
environment:
  - BACKEND_URL=http://192.168.50.120:8799
```

### é˜²ç«ç‰†è¨­å®š

**å¾Œç«¯æœå‹™å™¨**ï¼š
```bash
# é–‹æ”¾ç«¯å£ 8799 çµ¦å…§ç¶²
sudo ufw allow from 192.168.0.0/16 to any port 8799
```

**å‰ç«¯æœå‹™å™¨**ï¼š
```bash
# é–‹æ”¾ç«¯å£ 8496 å°å¤–
sudo ufw allow 8496
```

## ğŸ“š API ä½¿ç”¨æ–¹æ³•

### Web ç•Œé¢

è¨ªå• `http://140.96.170.75:8496` ä½¿ç”¨åœ–å½¢ç•Œé¢ä¸Šå‚³éŸ³é »/è¦–é »æ–‡ä»¶

### API èª¿ç”¨

#### 1. æ–‡ä»¶ä¸Šå‚³è½‰éŒ„ (æ”¯æ´æ ¼å¼ï¼šmp3, wav, ogg, flac, m4a, mp4)
```bash
curl -X POST http://140.96.170.75:8496/transcribe/file \
  -F "file=@your-audio.mp3" \
  -F "model=large-v3" \
  -F "timestamps=true"
```

#### 2. äºŒé€²åˆ¶æ•¸æ“šè½‰éŒ„
```bash
curl -X POST "http://140.96.170.75:8496/transcribe/binary?model=large-v3&ext=wav" \
  --data-binary "@your-audio.wav" \
  -H "Content-Type: application/octet-stream"
```

#### 3. ç²å–å¯ç”¨æ¨¡å‹
```bash
curl http://140.96.170.75:8496/models
```

#### 4. Python èª¿ç”¨ç¯„ä¾‹
```python
import requests

# æ–‡ä»¶ä¸Šå‚³è½‰éŒ„
with open("audio.mp3", "rb") as f:
    files = {"file": f}
    data = {
        "model": "large-v3",
        "timestamps": "true"
    }
    response = requests.post(
        "http://140.96.170.75:8496/transcribe/file",
        files=files,
        data=data
    )

result = response.json()
if result["success"]:
    print(f"è½‰éŒ„çµæœ: {result['text']}")
    print(f"è™•ç†æ™‚é–“: {result['process_time']:.2f}ç§’")
    
    # å¦‚æœæœ‰æ™‚é–“è»¸æ•¸æ“š
    if "segments" in result:
        for segment in result["segments"]:
            print(f"{segment['start']:.2f}s - {segment['end']:.2f}s: {segment['text']}")
else:
    print(f"éŒ¯èª¤: {result['error']}")
```

#### 5. JavaScript èª¿ç”¨ç¯„ä¾‹
```javascript
// æ–‡ä»¶ä¸Šå‚³è½‰éŒ„
async function transcribeAudio(file) {
    const formData = new FormData();
    formData.append('file', file);
    formData.append('model', 'large-v3');
    formData.append('timestamps', 'true');
    
    try {
        const response = await fetch('http://140.96.170.75:8496/transcribe/file', {
            method: 'POST',
            body: formData
        });
        
        const result = await response.json();
        
        if (result.success) {
            console.log('è½‰éŒ„çµæœ:', result.text);
            console.log('è™•ç†æ™‚é–“:', result.process_time + 'ç§’');
            
            // é¡¯ç¤ºæ™‚é–“è»¸æ•¸æ“š
            if (result.segments) {
                result.segments.forEach(segment => {
                    console.log(`${segment.start.toFixed(2)}s - ${segment.end.toFixed(2)}s: ${segment.text}`);
                });
            }
        } else {
            console.error('è½‰éŒ„å¤±æ•—:', result.error);
        }
    } catch (error) {
        console.error('è«‹æ±‚éŒ¯èª¤:', error);
    }
}

// ä½¿ç”¨ç¯„ä¾‹
document.getElementById('fileInput').addEventListener('change', function(e) {
    const file = e.target.files[0];
    if (file) {
        transcribeAudio(file);
    }
});
```

## âš™ï¸ é…ç½®é¸é …

### å¾Œç«¯é…ç½®

ç·¨è¼¯ `whisper_backend/waitress_server.py` ä¸­çš„è¨­å®šï¼š

```python
# æ›´æ”¹ç›£è½ç«¯å£
if __name__ == "__main__":
    serve(app, host="0.0.0.0", port=8799)

# æ›´æ”¹ GPU è¨­å‚™
device = "cuda:1"  # å¯æ”¹ç‚º "cuda:0" æˆ–å…¶ä»–GPU

# æ›´æ”¹é»˜èªæ¨¡å‹
default_model_name = "large-v3"  # å¯é¸: turbo, large-v3

# æ”¯æ´çš„æ–‡ä»¶æ ¼å¼
ALLOWED_EXTENSIONS = {'mp3', 'wav', 'ogg', 'flac', 'm4a', 'mp4'}
```

### å‰ç«¯é…ç½®

ç·¨è¼¯ `Whisper_frontend/docker-compose.yml`ï¼š

```yaml
environment:
  - BACKEND_URL=http://192.168.50.120:8799  # å¾Œç«¯æœå‹™å™¨åœ°å€
ports:
  - "8496:8499"  # å°å¤–ç«¯å£:å…§éƒ¨ç«¯å£
```

## ğŸ¯ åŠŸèƒ½ç‰¹è‰²

### æ”¯æ´æ ¼å¼
- **éŸ³é »æ ¼å¼**: MP3, WAV, OGG, FLAC, M4A
- **è¦–é »æ ¼å¼**: MP4 (è‡ªå‹•æå–éŸ³è»Œ)

### æ¨¡å‹æ”¯æ´
- **large-v3**: é«˜ç²¾åº¦æ¨¡å‹ï¼Œé©åˆé‡è¦å ´åˆ
- **turbo**: å¿«é€Ÿæ¨¡å‹ï¼Œé©åˆå³æ™‚è½‰éŒ„

### èªè¨€è™•ç†
- è‡ªå‹•è­˜åˆ¥ä¸­æ–‡èªéŸ³
- ç°¡é«”è½‰ç¹é«”ä¸­æ–‡è¼¸å‡º
- æ”¯æ´æ™‚é–“è»¸æ¨™è¨˜

### éšŠåˆ—è™•ç†
- æ”¯æ´ä¸¦ç™¼è«‹æ±‚éšŠåˆ—
- æ™ºèƒ½ä»»å‹™æ’ç¨‹
- GPU è³‡æºä¿è­·

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œ

1. **å‰ç«¯ç„¡æ³•é€£æ¥å¾Œç«¯**ï¼š
   ```bash
   # æª¢æŸ¥ç¶²çµ¡é€£é€šæ€§
   ping 192.168.50.120
   
   # æª¢æŸ¥ç«¯å£é–‹æ”¾
   telnet 192.168.50.120 8799
   
   # æª¢æŸ¥å¾Œç«¯æœå‹™ç‹€æ…‹
   curl http://192.168.50.120:8799/health
   ```

2. **GPU è¨˜æ†¶é«”ä¸è¶³**ï¼š
   ```bash
   # æª¢æŸ¥ GPU ç‹€æ…‹
   nvidia-smi
   
   # é‡å•Ÿå¾Œç«¯æœå‹™é‡‹æ”¾è¨˜æ†¶é«”
   pkill -f waitress_server.py
   python waitress_server.py
   ```

3. **éŸ³é »è½‰æ›å¤±æ•—**ï¼š
   ```bash
   # æª¢æŸ¥ FFmpeg å®‰è£
   ffmpeg -version
   
   # å®‰è£ FFmpeg (å¦‚æœæœªå®‰è£)
   sudo apt install ffmpeg
   ```

4. **Docker å®¹å™¨ç„¡æ³•å•Ÿå‹•**ï¼š
   ```bash
   # æª¢æŸ¥æ—¥èªŒ
   docker-compose logs whisper-trans
   
   # é‡å»ºå®¹å™¨
   docker-compose down
   docker-compose up --build -d
   ```

### æ—¥èªŒæª¢æŸ¥

**å‰ç«¯æ—¥èªŒ**ï¼š
```bash
# Docker æ—¥èªŒ
docker-compose logs -f whisper-trans

# æ–‡ä»¶æ—¥èªŒ
tail -f logs/app.log
```

**å¾Œç«¯æ—¥èªŒ**ï¼š
```bash
# ç›´æ¥é‹è¡Œçš„å¾Œç«¯æœƒåœ¨çµ‚ç«¯é¡¯ç¤ºæ—¥èªŒ
python waitress_server.py
```

## ğŸ“Š æ€§èƒ½å„ªåŒ–

### å¾Œç«¯å„ªåŒ–

1. **GPU è¨˜æ†¶é«”ç®¡ç†**ï¼š
   - ä½¿ç”¨é©ç•¶çš„æ¨¡å‹å¤§å°
   - å•Ÿç”¨ fp16 ç²¾åº¦ (CUDAç’°å¢ƒ)
   - å¯¦ç¾è«‹æ±‚éšŠåˆ—é¿å…ä½µç™¼è¡çª

2. **æ¨¡å‹å„ªåŒ–**ï¼š
   - é è¼‰å¸¸ç”¨æ¨¡å‹åˆ°è¨˜æ†¶é«”
   - ä½¿ç”¨ turbo æ¨¡å‹æå‡é€Ÿåº¦
   - è€ƒæ…®ä½¿ç”¨ whisper.cpp é€²ä¸€æ­¥å„ªåŒ–

### å‰ç«¯å„ªåŒ–

1. **é€£æ¥æ± **ï¼š
   - ä½¿ç”¨ requests.Session() é‡ç”¨é€£æ¥
   - è¨­ç½®é©ç•¶çš„è¶…æ™‚æ™‚é–“

2. **ç·©å­˜**ï¼š
   - è€ƒæ…®æ·»åŠ  Redis ç·©å­˜è½‰éŒ„çµæœ
   - å¯¦ç¾æ–‡ä»¶å“ˆå¸Œçš„çµæœç·©å­˜

## ğŸ›¡ï¸ å®‰å…¨æ³¨æ„äº‹é …

1. **æ–‡ä»¶ä¸Šå‚³é™åˆ¶**ï¼š
   - é™åˆ¶æ–‡ä»¶å¤§å° (å»ºè­° 100MB ä»¥ä¸‹)
   - æª¢æŸ¥æ–‡ä»¶é¡å‹å’Œå…§å®¹
   - å®šæœŸæ¸…ç†è‡¨æ™‚æ–‡ä»¶

2. **ç¶²çµ¡å®‰å…¨**ï¼š
   - å¾Œç«¯æœå‹™åƒ…é–‹æ”¾çµ¦å…§ç¶²
   - å‰ç«¯æœå‹™é…ç½®é©ç•¶çš„é˜²ç«ç‰†è¦å‰‡
   - è€ƒæ…®æ·»åŠ  API èªè­‰æ©Ÿåˆ¶

## ğŸ“„ æˆæ¬Š

æœ¬é …ç›®åŸºæ–¼é–‹æºå”è­°ï¼Œè«‹éµå¾ªç›¸é—œçµ„ä»¶çš„æˆæ¬Šæ¢æ¬¾ï¼š
- [OpenAI Whisper](https://github.com/openai/whisper)
- [Flask](https://flask.palletsprojects.com/)
- [Docker](https://www.docker.com/)

## ğŸ¤ è²¢ç»

æ­¡è¿æäº¤ Issue å’Œ Pull Request ä¾†æ”¹é€²é€™å€‹é …ç›®ã€‚

## ğŸ“ è¯çµ¡

å¦‚æœ‰å•é¡Œæˆ–å»ºè­°ï¼Œè«‹è¯ç¹«é …ç›®ç¶­è­·è€…ã€‚ 